{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a535903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
      "\n",
      "SUMMARY of loaded models/artifacts:\n",
      " CNN: loaded\n",
      " Hybrid: loaded\n",
      " Sklearn: loaded\n",
      " CNN label encoder: loaded\n",
      " Sklearn label encoder: loaded\n",
      " Sklearn scaler: loaded\n",
      " Hybrid scaler: loaded\n",
      " Hybrid label encoder: loaded\n",
      " scanner_fps: loaded\n",
      " fp_keys: 11 entries\n",
      "\n",
      "CNN -> EpsonV39-2 0.1250104159116745\n",
      "Hybrid -> Canon120-2 0.7965758442878723\n",
      "Sklearn -> Canon120-2 0.34606994353863874\n",
      "Ensemble vote -> Canon120-2\n",
      "Notes:\n",
      " - Hybrid features padded/truncated 31 -> 27\n",
      "\n",
      "RETURNED:\n",
      " {'cnn': {'label': 'EpsonV39-2', 'index': 8, 'confidence': 0.1250104159116745, 'probs': [0.10856552422046661, 0.11851204931735992, 0.09183135628700256, 0.11133231222629547, 0.12217840552330017, 0.058213114738464355, 0.06529180705547333, 0.11221949756145477, 0.1250104159116745, 0.05955833941698074, 0.027287188917398453]}, 'hybrid': {'label': 'Canon120-2', 'index': 1, 'confidence': 0.7965758442878723, 'probs': [0.1968243271112442, 0.7965758442878723, 0.000478045258205384, 1.843076091745388e-08, 0.0003526912478264421, 3.806229962287355e-14, 6.000981656040238e-11, 0.00484210392460227, 0.0009269571164622903, 6.894973657405317e-16, 1.1405539845910084e-09]}, 'sklearn': {'label': 'Canon120-2', 'index': 1, 'confidence': 0.34606994353863874, 'probs': [0.2990421481466562, 0.34606994353863874, 0.21547725487052347, 0.03139344106265941, 0.03844112113380894, 8.744312987507644e-08, 8.407836115147307e-07, 0.03541506841057942, 0.033173457918900175, 0.0005063945724248128, 0.00048024211906732563]}, 'ensemble_vote': 'Canon120-2', 'notes': ['Hybrid features padded/truncated 31 -> 27'], 'selected_model': 'all'}\n"
     ]
    }
   ],
   "source": [
    "# backend/inference/singleimage_prediction.py\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "from collections import Counter\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.fft import fft2, fftshift\n",
    "from skimage.feature import local_binary_pattern, graycomatrix, graycoprops\n",
    "from skimage import img_as_ubyte\n",
    "import traceback\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# ----------------- Utilities -----------------\n",
    "def _to_py(x):\n",
    "    if isinstance(x, np.generic):\n",
    "        return x.item()\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return _to_py(x.tolist())\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return [_to_py(v) for v in x]\n",
    "    if isinstance(x, dict):\n",
    "        return {str(k): _to_py(v) for k, v in x.items()}\n",
    "    return x\n",
    "\n",
    "# ----------------- Paths & try-locate -----------------\n",
    "MODELS_DIRS_TO_TRY = [\n",
    "    r\"D:\\Infosys_AI-Tracefinder\\Notebooks\",\n",
    "    r\"D:\\Infosys_AI-Tracefinder\\Output\",\n",
    "    os.path.join(os.getcwd(), \"models\"),\n",
    "    os.getcwd()\n",
    "]\n",
    "\n",
    "def find_file(filename):\n",
    "    for d in MODELS_DIRS_TO_TRY:\n",
    "        p = os.path.join(d, filename)\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "# default names (match your notebook outputs)\n",
    "CNN_LE_FNAME = \"cnn_label_encoder.pkl\"\n",
    "CNN_MODEL_FNAME = \"cnn_residual_best.keras\"\n",
    "HYB_MODEL_FNAME = \"scanner_hybrid_best.keras\"\n",
    "SKL_MODEL_FNAME = \"best_model.pkl\"\n",
    "SKL_SCALER_FNAME = \"scaler.pkl\"\n",
    "SKL_LE_FNAME = \"label_encoder.pkl\"\n",
    "HYB_SCALER_FNAME = \"hybrid_feat_scaler.pkl\"\n",
    "HYB_LE_FNAME = \"hybrid_label_encoder.pkl\"\n",
    "FPICKS_FNAME = \"scanner_fingerprints.pkl\"\n",
    "FP_KEYS = \"fp_keys.npy\"\n",
    "\n",
    "# ----------------- Basic feature (CSV) helpers -----------------\n",
    "def guess_resolution_from_path(path):\n",
    "    p = path.replace(\"\\\\\",\"/\").lower()\n",
    "    if \"/150\" in p or \"_150\" in p or \"/150/\" in p:\n",
    "        return 150\n",
    "    if \"/300\" in p or \"_300\" in p or \"/300/\" in p:\n",
    "        return 300\n",
    "    return 0\n",
    "\n",
    "def compute_basic_features(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Cannot read image: {img_path}\")\n",
    "    if img.ndim == 3:\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        img_gray = img.copy()\n",
    "\n",
    "    h, w = img_gray.shape[:2]\n",
    "    aspect_ratio = float(w)/float(h) if h!=0 else 0.0\n",
    "    file_size_kb = float(os.path.getsize(img_path))/1024.0\n",
    "\n",
    "    imgf = img_gray.astype(np.float32)\n",
    "    maxv = imgf.max() if imgf.max()>0 else 1.0\n",
    "    imgn = imgf / maxv\n",
    "\n",
    "    mean_intensity = float(imgn.mean())\n",
    "    std_intensity = float(imgn.std())\n",
    "\n",
    "    flat = imgn.flatten()\n",
    "    skewness = float(skew(flat)) if flat.size>0 else 0.0\n",
    "    kurt = float(kurtosis(flat)) if flat.size>0 else 0.0\n",
    "\n",
    "    hist, _ = np.histogram(flat, bins=256, range=(0.0,1.0), density=True)\n",
    "    hist = hist + 1e-12\n",
    "    entropy = float(-np.sum(hist * np.log2(hist)))\n",
    "\n",
    "    edges = cv2.Canny((imgn*255).astype(np.uint8), 100, 200)\n",
    "    edge_density = float(edges.astype(bool).sum()) / (w*h) if (w*h)>0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"width\": int(w),\n",
    "        \"height\": int(h),\n",
    "        \"aspect_ratio\": float(aspect_ratio),\n",
    "        \"file_size_kb\": float(file_size_kb),\n",
    "        \"mean_intensity\": float(mean_intensity),\n",
    "        \"std_intensity\": float(std_intensity),\n",
    "        \"skewness\": float(skewness),\n",
    "        \"kurtosis\": float(kurt),\n",
    "        \"entropy\": float(entropy),\n",
    "        \"edge_density\": float(edge_density)\n",
    "    }\n",
    "\n",
    "def make_feature_vector(image_path):\n",
    "    # CSV/basic features (11)\n",
    "    res_level = guess_resolution_from_path(image_path)\n",
    "    feats = compute_basic_features(image_path)\n",
    "    ordered = [\n",
    "        float(res_level),\n",
    "        float(feats[\"width\"]),\n",
    "        float(feats[\"height\"]),\n",
    "        float(feats[\"aspect_ratio\"]),\n",
    "        float(feats[\"file_size_kb\"]),\n",
    "        float(feats[\"mean_intensity\"]),\n",
    "        float(feats[\"std_intensity\"]),\n",
    "        float(feats[\"skewness\"]),\n",
    "        float(feats[\"kurtosis\"]),\n",
    "        float(feats[\"entropy\"]),\n",
    "        float(feats[\"edge_density\"])\n",
    "    ]\n",
    "    return np.array(ordered, dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "# ----------------- Residual (CNN) helper -----------------\n",
    "def make_residual_image(image_path, target_size=(256,256)):\n",
    "    import pywt\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Cannot read image: {image_path}\")\n",
    "    if img.ndim == 3:\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        img_gray = img.copy()\n",
    "    img_resized = cv2.resize(img_gray, target_size, interpolation=cv2.INTER_AREA).astype(np.float32)\n",
    "    maxv = img_resized.max() if img_resized.max()>0 else 1.0\n",
    "    imgn = img_resized / maxv\n",
    "    coeffs = pywt.dwt2(imgn, \"haar\")\n",
    "    cA, (cH, cV, cD) = coeffs\n",
    "    cH[:] = 0; cV[:] = 0; cD[:] = 0\n",
    "    den = pywt.idwt2((cA, (cH, cV, cD)), 'haar')\n",
    "    res = imgn - den\n",
    "    m = np.max(np.abs(res)) if np.max(np.abs(res))>0 else 1.0\n",
    "    res = (res / m).astype(np.float32)\n",
    "    return np.expand_dims(res, axis=(0,-1))   # shape (1,256,256,1)\n",
    "\n",
    "# ----------------- Hybrid features builders (31) -----------------\n",
    "def corr2d(a, b):\n",
    "    a = a.astype(np.float32).ravel(); b = b.astype(np.float32).ravel()\n",
    "    a -= a.mean(); b -= b.mean()\n",
    "    denom = np.linalg.norm(a)*np.linalg.norm(b)\n",
    "    return float((a @ b)/denom) if denom>0 else 0.0\n",
    "\n",
    "def fft_radial_energy(img, K=6, log_scale=True):\n",
    "    f = fftshift(fft2(img))\n",
    "    mag = np.abs(f)\n",
    "    if log_scale:\n",
    "        mag = 20 * np.log1p(mag)\n",
    "    h,w = mag.shape\n",
    "    cy, cx = h//2, w//2\n",
    "    yy, xx = np.ogrid[:h, :w]\n",
    "    r = np.sqrt((yy-cy)**2 + (xx-cx)**2)\n",
    "    bins = np.linspace(0, r.max()+1e-12, K+1)\n",
    "    feats = []\n",
    "    for i in range(K):\n",
    "        mask = (r >= bins[i]) & (r < bins[i+1])\n",
    "        sel = mag[mask]\n",
    "        feats.append(float(np.mean(sel)) if sel.size else 0.0)\n",
    "    return feats\n",
    "\n",
    "def lbp_hist_safe(img, P=8, R=1.0, bins=None):\n",
    "    if bins is None:\n",
    "        bins = P + 2\n",
    "    rng = float(img.max() - img.min())\n",
    "    if rng < 1e-12:\n",
    "        g8 = np.zeros_like(img, dtype=np.uint8)\n",
    "    else:\n",
    "        g8 = ((img - img.min())/(rng + 1e-12) * 255).astype(np.uint8)\n",
    "    codes = local_binary_pattern(g8, P, R, method=\"uniform\")\n",
    "    hist, _ = np.histogram(codes.ravel(), bins=np.arange(bins+1), density=True)\n",
    "    return hist.astype(np.float32).tolist()\n",
    "\n",
    "def glcm_features(img, distances=(1,), angles=(0,)):\n",
    "    try:\n",
    "        g8 = img_as_ubyte((img - img.min())/((img.max()-img.min())+1e-12))\n",
    "    except Exception:\n",
    "        g8 = img_as_ubyte(np.clip(img,0.0,1.0))\n",
    "    try:\n",
    "        glcm = graycomatrix(g8, distances=distances, angles=angles, levels=256, symmetric=True, normed=True)\n",
    "        return [float(graycoprops(glcm,p)[0,0]) for p in (\"contrast\",\"homogeneity\",\"energy\",\"correlation\")]\n",
    "    except Exception:\n",
    "        return [0.0,0.0,0.0,0.0]\n",
    "\n",
    "def make_hybrid_feature_vector_from_residual(residual, scanner_fps, fp_keys):\n",
    "    # residual expected 2D (H,W) float\n",
    "    # returns (1, N) where N is typically 31 in your new pipeline\n",
    "    v_corr = [corr2d(residual, scanner_fps.get(k, np.zeros_like(residual))) for k in fp_keys]   # 11\n",
    "    v_fft = fft_radial_energy(residual, K=6, log_scale=True)                                   # 6\n",
    "    v_lbp = lbp_hist_safe(residual, P=8, R=1.0, bins=10)                                       # 10\n",
    "    v_glcm = glcm_features(residual, distances=(1,), angles=(0,))                              # 4\n",
    "    feat_vec = v_corr + v_fft + v_lbp + v_glcm\n",
    "    return np.array(feat_vec, dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "def make_hybrid_feature_vector(image_path, scanner_fps, fp_keys, target_size=(256,256)):\n",
    "    # convenience: compute residual then features\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Unreadable image for hybrid features.\")\n",
    "    if img.ndim == 3:\n",
    "        g = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        g = img.copy()\n",
    "    g = cv2.resize(g, target_size).astype(np.float32)\n",
    "    g /= (g.max() if g.max()>0 else 1.0)\n",
    "    # simple wavelet denoise (safe)\n",
    "    try:\n",
    "        import pywt\n",
    "        cA,(cH,cV,cD) = pywt.dwt2(g, \"haar\")\n",
    "        cH[:] = cV[:] = cD[:] = 0\n",
    "        den = pywt.idwt2((cA,(cH,cV,cD)),\"haar\")\n",
    "        den = cv2.resize(den, target_size)\n",
    "    except Exception:\n",
    "        den = np.zeros_like(g)\n",
    "    res = (g - den).astype(np.float32)\n",
    "    m = np.max(np.abs(res)) if np.max(np.abs(res))>0 else 1.0\n",
    "    res = (res / m).astype(np.float32)\n",
    "    return make_hybrid_feature_vector_from_residual(res, scanner_fps, fp_keys)\n",
    "\n",
    "# ----------------- Load artifacts -----------------\n",
    "# find files\n",
    "cnn_le_path = find_file(CNN_LE_FNAME)\n",
    "cnn_model_path = find_file(CNN_MODEL_FNAME)\n",
    "hyb_model_path = find_file(HYB_MODEL_FNAME)\n",
    "skl_model_path = find_file(SKL_MODEL_FNAME)\n",
    "skl_scaler_path = find_file(SKL_SCALER_FNAME)\n",
    "skl_le_path = find_file(SKL_LE_FNAME)\n",
    "hyb_scaler_path = find_file(HYB_SCALER_FNAME)\n",
    "hyb_le_path = find_file(HYB_LE_FNAME)\n",
    "fpicks_path = find_file(FPICKS_FNAME)\n",
    "fp_keys_path = find_file(FP_KEYS)\n",
    "\n",
    "# load encoders / models (best-effort)\n",
    "cnn_le = None\n",
    "if cnn_le_path:\n",
    "    with open(cnn_le_path,\"rb\") as f: cnn_le = pickle.load(f)\n",
    "\n",
    "cnn = None\n",
    "if cnn_model_path:\n",
    "    try:\n",
    "        cnn = keras.models.load_model(cnn_model_path)\n",
    "        try: cnn.predict(np.zeros((1,256,256,1),dtype=np.float32))\n",
    "        except: pass\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load CNN:\", e); cnn = None\n",
    "\n",
    "hybrid = None\n",
    "if hyb_model_path:\n",
    "    try:\n",
    "        hybrid = keras.models.load_model(hyb_model_path)\n",
    "        # warm-up: best effort\n",
    "        try:\n",
    "            inps=[]\n",
    "            for inp in hybrid.inputs:\n",
    "                s = inp.shape\n",
    "                if len(s)==4: inps.append(np.zeros((1,256,256,1),dtype=np.float32))\n",
    "                elif len(s)==2: inps.append(np.zeros((1, max(1, int(s[1]) if s[1] is not None else 27)), dtype=np.float32))\n",
    "                else: inps.append(np.zeros((1,1),dtype=np.float32))\n",
    "            hybrid.predict(inps)\n",
    "        except Exception:\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load Hybrid:\", e); hybrid = None\n",
    "\n",
    "sklearn_model = None\n",
    "skl_scaler = None\n",
    "skl_le = None\n",
    "if skl_model_path:\n",
    "    try: sklearn_model = joblib.load(skl_model_path)\n",
    "    except Exception as e: print(\"Failed to load sklearn model:\", e); sklearn_model = None\n",
    "if skl_scaler_path:\n",
    "    try: skl_scaler = joblib.load(skl_scaler_path)\n",
    "    except Exception as e: print(\"Failed to load sklearn scaler:\", e); skl_scaler = None\n",
    "if skl_le_path:\n",
    "    try: skl_le = joblib.load(skl_le_path)\n",
    "    except Exception as e: print(\"Failed to load sklearn label encoder:\", e); skl_le = None\n",
    "\n",
    "hyb_scaler = None\n",
    "hyb_le = None\n",
    "if hyb_scaler_path:\n",
    "    try: hyb_scaler = joblib.load(hyb_scaler_path)\n",
    "    except Exception as e: print(\"Failed to load hybrid scaler:\", e); hyb_scaler = None\n",
    "if hyb_le_path:\n",
    "    try: hyb_le = joblib.load(hyb_le_path)\n",
    "    except Exception as e: print(\"Failed to load hybrid label encoder:\", e); hyb_le = None\n",
    "\n",
    "scanner_fps = {}\n",
    "fp_keys = []\n",
    "if fpicks_path:\n",
    "    try:\n",
    "        with open(fpicks_path,\"rb\") as f: scanner_fps = pickle.load(f)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load scanner_fingerprints:\", e)\n",
    "if fp_keys_path:\n",
    "    try:\n",
    "        fp_keys = list(np.load(fp_keys_path).tolist())\n",
    "    except Exception as e:\n",
    "        fp_keys = sorted(list(scanner_fps.keys()))\n",
    "\n",
    "print(\"\\nSUMMARY of loaded models/artifacts:\")\n",
    "print(\" CNN:\", \"loaded\" if cnn is not None else \"missing\")\n",
    "print(\" Hybrid:\", \"loaded\" if hybrid is not None else \"missing\")\n",
    "print(\" Sklearn:\", \"loaded\" if sklearn_model is not None else \"missing\")\n",
    "print(\" CNN label encoder:\", \"loaded\" if cnn_le is not None else \"missing\")\n",
    "print(\" Sklearn label encoder:\", \"loaded\" if skl_le is not None else \"missing\")\n",
    "print(\" Sklearn scaler:\", \"loaded\" if skl_scaler is not None else \"missing\")\n",
    "print(\" Hybrid scaler:\", \"loaded\" if hyb_scaler is not None else \"missing\")\n",
    "print(\" Hybrid label encoder:\", \"loaded\" if hyb_le is not None else \"missing\")\n",
    "print(\" scanner_fps:\", \"loaded\" if scanner_fps else \"missing\")\n",
    "print(\" fp_keys:\", len(fp_keys), \"entries\\n\")\n",
    "\n",
    "# ----------------- Prediction main -----------------\n",
    "def predict_single_image(image_path, model_choice=\"all\", verbose=True):\n",
    "    \"\"\"\n",
    "    model_choice: \"cnn\", \"hybrid\", \"sklearn\", or \"all\"\n",
    "    returns dict with per-model label/confidence/probs and 'selected_model' field.\n",
    "    \"\"\"\n",
    "    results = {\"cnn\": None, \"hybrid\": None, \"sklearn\": None, \"ensemble_vote\": None, \"notes\": [], \"selected_model\": model_choice}\n",
    "\n",
    "    # 1) residual image for CNN/hybrid image input\n",
    "    try:\n",
    "        img_res = make_residual_image(image_path, target_size=(256,256))   # shape (1,256,256,1)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Residual creation failed: {e}\")\n",
    "\n",
    "    # 2) CSV/basic features (11)\n",
    "    try:\n",
    "        feats_basic = make_feature_vector(image_path)   # (1,11)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Basic feature extraction failed: {e}\")\n",
    "\n",
    "    # Run CNN?\n",
    "    if model_choice in (\"cnn\",\"all\") and cnn is not None:\n",
    "        try:\n",
    "            proba = cnn.predict(img_res, verbose=0)[0]\n",
    "            idx = int(np.argmax(proba))\n",
    "            label = cnn_le.inverse_transform([idx])[0] if cnn_le is not None else str(idx)\n",
    "            results[\"cnn\"] = {\"label\": label, \"index\": idx, \"confidence\": float(proba[idx]), \"probs\": _to_py(proba)}\n",
    "            if verbose: print(\"CNN ->\", label, float(proba[idx]))\n",
    "        except Exception as e:\n",
    "            results[\"cnn\"] = {\"error\": str(e)}\n",
    "            results[\"notes\"].append(\"CNN prediction failed: \" + str(e))\n",
    "\n",
    "    # Run Hybrid?\n",
    "    if model_choice in (\"hybrid\",\"all\") and hybrid is not None:\n",
    "        try:\n",
    "            # build raw 31-feature vector from residual (uses scanner_fps / fp_keys)\n",
    "            try:\n",
    "                # prefer computing features from residual image to match pipeline\n",
    "                residual_2d = np.squeeze(img_res[0,:,:,0])\n",
    "                raw_feat = make_hybrid_feature_vector_from_residual(residual_2d, scanner_fps, fp_keys)  # shape (1,31 typically)\n",
    "            except Exception:\n",
    "                # fallback compute from image path\n",
    "                raw_feat = make_hybrid_feature_vector(image_path, scanner_fps, fp_keys)\n",
    "\n",
    "            # detect expected length (prefer hyb_scaler.n_features_in_, then hybrid model input)\n",
    "            expected = None\n",
    "            if hyb_scaler is not None:\n",
    "                expected = getattr(hyb_scaler, \"n_features_in_\", None) or getattr(hyb_scaler, \"n_features_in\", None)\n",
    "            if expected is None and hybrid is not None:\n",
    "                try:\n",
    "                    for inp in hybrid.inputs:\n",
    "                        s = inp.shape\n",
    "                        if len(s) == 2 and s[1] is not None:\n",
    "                            expected = int(s[1]); break\n",
    "                except Exception:\n",
    "                    expected = None\n",
    "            if expected is None:\n",
    "                expected = raw_feat.shape[1]  # fallback\n",
    "\n",
    "            # pad/truncate to expected\n",
    "            if raw_feat.shape[1] != expected:\n",
    "                adj = np.zeros((1, int(expected)), dtype=np.float32)\n",
    "                take = min(raw_feat.shape[1], int(expected))\n",
    "                adj[0, :take] = raw_feat[0, :take]\n",
    "                feat_for_scaler = adj\n",
    "                results[\"notes\"].append(f\"Hybrid features padded/truncated {raw_feat.shape[1]} -> {expected}\")\n",
    "            else:\n",
    "                feat_for_scaler = raw_feat\n",
    "\n",
    "            # scale if scaler present\n",
    "            try:\n",
    "                if hyb_scaler is not None:\n",
    "                    # hyb_scaler may expect dataframe with feature_names_in_ -> handle both\n",
    "                    if hasattr(hyb_scaler, \"feature_names_in_\"):\n",
    "                        import pandas as pd\n",
    "                        cols = list(getattr(hyb_scaler, \"feature_names_in_\"))\n",
    "                        df = pd.DataFrame(np.zeros((1,len(cols))), columns=cols, dtype=np.float32)\n",
    "                        for i in range(min(feat_for_scaler.shape[1], len(cols))):\n",
    "                            df.iloc[0,i] = feat_for_scaler[0,i]\n",
    "                        feat_scaled = hyb_scaler.transform(df)\n",
    "                    else:\n",
    "                        feat_scaled = hyb_scaler.transform(feat_for_scaler)\n",
    "                else:\n",
    "                    feat_scaled = feat_for_scaler\n",
    "            except Exception as e:\n",
    "                results[\"notes\"].append(\"Hybrid scaler transform failed: \" + str(e))\n",
    "                feat_scaled = feat_for_scaler\n",
    "\n",
    "            # prepare inputs in correct order: put image where hybrid expects image input\n",
    "            inputs = []\n",
    "            img_input_index = None\n",
    "            for i, inp in enumerate(hybrid.inputs):\n",
    "                s = inp.shape\n",
    "                if len(s) == 4:\n",
    "                    img_input_index = i\n",
    "                    break\n",
    "            if img_input_index is None:\n",
    "                img_input_index = 0\n",
    "            for i, inp in enumerate(hybrid.inputs):\n",
    "                if i == img_input_index:\n",
    "                    inputs.append(img_res)\n",
    "                else:\n",
    "                    inputs.append(np.asarray(feat_scaled, dtype=np.float32))\n",
    "\n",
    "            proba_h = hybrid.predict(inputs, verbose=0)\n",
    "            if isinstance(proba_h, list): proba_h = proba_h[0]\n",
    "            proba_h = np.asarray(proba_h)[0]\n",
    "            idx_h = int(np.argmax(proba_h))\n",
    "            if hyb_le is not None:\n",
    "                label_h = hyb_le.inverse_transform([idx_h])[0]\n",
    "            else:\n",
    "                label_h = (cnn_le.inverse_transform([idx_h])[0] if cnn_le is not None else str(idx_h))\n",
    "            results[\"hybrid\"] = {\"label\": label_h, \"index\": idx_h, \"confidence\": float(proba_h[idx_h]), \"probs\": _to_py(proba_h)}\n",
    "            if verbose: print(\"Hybrid ->\", label_h, float(proba_h[idx_h]))\n",
    "        except Exception as e:\n",
    "            tb = traceback.format_exc()\n",
    "            results[\"hybrid\"] = {\"error\": str(e), \"trace\": tb}\n",
    "            results[\"notes\"].append(\"Hybrid processing error: \" + str(e))\n",
    "\n",
    "    # Run Sklearn?\n",
    "    if model_choice in (\"sklearn\",\"all\") and sklearn_model is not None and skl_scaler is not None:\n",
    "        try:\n",
    "            feats_skl = feats_basic.copy()   # (1,11)\n",
    "            expected_skl = int(getattr(skl_scaler, \"n_features_in_\", feats_skl.shape[1]))\n",
    "            if feats_skl.shape[1] != expected_skl:\n",
    "                padded = np.zeros((1, expected_skl), dtype=np.float32)\n",
    "                padded[0, :feats_skl.shape[1]] = feats_skl[0]\n",
    "                feats_skl = padded\n",
    "                results[\"notes\"].append(f\"Sklearn features padded {feats_basic.shape[1]} -> {expected_skl}\")\n",
    "            # feature_names_in_ handling\n",
    "            if hasattr(skl_scaler, \"feature_names_in_\"):\n",
    "                import pandas as pd\n",
    "                cols = list(getattr(skl_scaler, \"feature_names_in_\"))\n",
    "                df = pd.DataFrame(np.zeros((1,len(cols))), columns=cols, dtype=np.float32)\n",
    "                for i in range(min(feats_skl.shape[1], len(cols))):\n",
    "                    df.iloc[0,i] = feats_skl[0,i]\n",
    "                Xs = skl_scaler.transform(df)\n",
    "            else:\n",
    "                Xs = skl_scaler.transform(feats_skl)\n",
    "            pred = sklearn_model.predict(Xs)\n",
    "            prob = None\n",
    "            try:\n",
    "                prob = sklearn_model.predict_proba(Xs)[0]\n",
    "                conf = float(np.max(prob))\n",
    "            except Exception:\n",
    "                conf = None\n",
    "            label_s = skl_le.inverse_transform(pred)[0] if skl_le is not None else str(pred[0])\n",
    "            results[\"sklearn\"] = {\"label\": label_s, \"index\": int(pred[0]), \"confidence\": conf, \"probs\": _to_py(prob)}\n",
    "            if verbose: print(\"Sklearn ->\", label_s, conf)\n",
    "        except Exception as e:\n",
    "            results[\"sklearn\"] = {\"error\": str(e)}\n",
    "            results[\"notes\"].append(\"Sklearn prediction failed: \" + str(e))\n",
    "    else:\n",
    "        if model_choice in (\"sklearn\",\"all\") and (sklearn_model is None or skl_scaler is None):\n",
    "            results[\"notes\"].append(\"sklearn model or scaler missing; skipped sklearn prediction.\")\n",
    "\n",
    "    # Ensemble vote (simple majority among available labels)\n",
    "    votes = []\n",
    "    for k in (\"cnn\",\"hybrid\",\"sklearn\"):\n",
    "        v = results.get(k)\n",
    "        if v and isinstance(v, dict) and (\"label\" in v):\n",
    "            votes.append(v[\"label\"])\n",
    "    results[\"ensemble_vote\"] = None if len(votes)==0 else Counter(votes).most_common(1)[0][0]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Ensemble vote ->\", results[\"ensemble_vote\"])\n",
    "        if len(results[\"notes\"])>0:\n",
    "            print(\"Notes:\")\n",
    "            for n in results[\"notes\"]:\n",
    "                print(\" -\", n)\n",
    "\n",
    "    return _to_py(results)\n",
    "\n",
    "# ---------------- Example usage -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    # quick local test (change path)\n",
    "    path = r\"D:\\Infosys_AI-Tracefinder\\Data\\Wikipedia\\Canon120-1\\150\\s1_1.tif\"\n",
    "    out = predict_single_image(path, model_choice=\"all\", verbose=True)\n",
    "    print(\"\\nRETURNED:\\n\", out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
