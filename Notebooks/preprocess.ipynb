{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78446825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORK: D:\\Infosys_AI-Tracefinder\\Notebooks\n",
      "OFFICIAL_DIR exists: True\n",
      "WIKI_DIR exists: True\n",
      "FLATFIELD_DIR exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pywt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import wiener as scipy_wiener\n",
    "from scipy.fft import fft2, fftshift\n",
    "from skimage.feature import local_binary_pattern, graycomatrix, graycoprops\n",
    "from skimage import img_as_ubyte\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# ---------------- CONFIG (edit these before running) ----------------\n",
    "WORK = r\"D:\\Infosys_AI-Tracefinder\\Notebooks\"   # where outputs will be written\n",
    "OFFICIAL_DIR = r\"D:\\Infosys_AI-Tracefinder\\Data\\Official\"\n",
    "WIKI_DIR     = r\"D:\\Infosys_AI-Tracefinder\\Data\\Wikipedia\"\n",
    "FLATFIELD_DIR= r\"D:\\Infosys_AI-Tracefinder\\Data\\Flatfield\"   # optional\n",
    "OUTPUT_DIR   = WORK\n",
    "IMG_SIZE     = (256, 256)\n",
    "DENOISE_METHOD = \"wavelet\"   # \"wavelet\" (mentor) or \"wiener\"\n",
    "VALID_EXTS = ('.tif', '.tiff', '.png', '.jpg', '.jpeg')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"WORK:\", WORK)\n",
    "print(\"OFFICIAL_DIR exists:\", os.path.exists(OFFICIAL_DIR))\n",
    "print(\"WIKI_DIR exists:\", os.path.exists(WIKI_DIR))\n",
    "print(\"FLATFIELD_DIR exists:\", os.path.exists(FLATFIELD_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e57754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- helpers ----------------\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "    print(\"Saved:\", path)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def safe_listdir(p):\n",
    "    try:\n",
    "        return os.listdir(p)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "\n",
    "# ---------------- denoise / preprocessing ----------------\n",
    "def to_gray(img):\n",
    "    if img is None:\n",
    "        return None\n",
    "    if img.ndim == 3:\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img\n",
    "\n",
    "def resize_to(img, size=IMG_SIZE):\n",
    "    # size expected (w,h) for cv2.resize when passed as tuple\n",
    "    return cv2.resize(img, size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def normalize_img(img):\n",
    "    # keep simple stable scaling: divide by max or by 255 if max==0\n",
    "    arr = img.astype(np.float32)\n",
    "    m = arr.max() if arr.max() > 0 else 255.0\n",
    "    return arr / float(m)\n",
    "\n",
    "def denoise_wavelet_safe(img, wavelet='db4', level=2):\n",
    "    arr = np.asarray(img, dtype=np.float32)\n",
    "    try:\n",
    "        coeffs = pywt.wavedec2(arr, wavelet=wavelet, level=level)\n",
    "    except Exception:\n",
    "        return arr.copy()\n",
    "    # robust MAD thresholding\n",
    "    details = []\n",
    "    for d in coeffs[1:]:\n",
    "        for comp in d:\n",
    "            details.append(np.ravel(np.nan_to_num(comp)))\n",
    "    if len(details) == 0:\n",
    "        return arr.copy()\n",
    "    details = np.concatenate(details)\n",
    "    mad = np.median(np.abs(details - np.median(details)))\n",
    "    sigma = mad / 0.6745 if mad > 0 else 0.0\n",
    "    uthresh = sigma * np.sqrt(2 * np.log(arr.size + 1e-12))\n",
    "    new_coeffs = [coeffs[0]]\n",
    "    for d in coeffs[1:]:\n",
    "        new_level = tuple(pywt.threshold(np.nan_to_num(comp), value=uthresh, mode='soft') for comp in d)\n",
    "        new_coeffs.append(new_level)\n",
    "    den = pywt.waverec2(new_coeffs, wavelet)\n",
    "    den = den[:arr.shape[0], :arr.shape[1]]\n",
    "    return np.nan_to_num(den, nan=arr).astype(np.float32)\n",
    "\n",
    "def denoise_mentor(img, method=DENOISE_METHOD):\n",
    "    if method == \"wiener\":\n",
    "        return scipy_wiener(img, mysize=(5,5)).astype(np.float32)\n",
    "    return denoise_wavelet_safe(img, wavelet='db4', level=2)\n",
    "\n",
    "def preprocess_image_path(path, method=DENOISE_METHOD, size=IMG_SIZE):\n",
    "    \"\"\"\n",
    "    Returns residual image (H,W) float32 normalized by max abs -> range approx [-1,1]\n",
    "    \"\"\"\n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = to_gray(img)\n",
    "    img = resize_to(img, size=size)\n",
    "    img = normalize_img(img)\n",
    "    den = denoise_mentor(img, method=method)\n",
    "    residual = (img - den).astype(np.float32)\n",
    "    # normalize residual by its max abs to keep values stable (-1..1)\n",
    "    m = np.max(np.abs(residual)) if np.max(np.abs(residual)) > 0 else 1.0\n",
    "    return (residual / m).astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "647ec860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- helpers ----------------\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "    print(\"Saved:\", path)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def safe_listdir(p):\n",
    "    try:\n",
    "        return os.listdir(p)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# ---------------- denoise / preprocessing ----------------\n",
    "def to_gray(img):\n",
    "    if img is None:\n",
    "        return None\n",
    "    if img.ndim == 3:\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img\n",
    "\n",
    "def resize_to(img, size=IMG_SIZE):\n",
    "    return cv2.resize(img, size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def normalize_img(img):\n",
    "    # keep simple stable scaling: divide by max or by 255 if max==0\n",
    "    arr = img.astype(np.float32)\n",
    "    m = arr.max() if arr.max() > 0 else 255.0\n",
    "    return arr / float(m)\n",
    "\n",
    "def denoise_wavelet_safe(img, wavelet='db4', level=2):\n",
    "    arr = np.asarray(img, dtype=np.float32)\n",
    "    try:\n",
    "        coeffs = pywt.wavedec2(arr, wavelet=wavelet, level=level)\n",
    "    except Exception:\n",
    "        return arr.copy()\n",
    "    # robust MAD thresholding\n",
    "    details = []\n",
    "    for d in coeffs[1:]:\n",
    "        for comp in d:\n",
    "            details.append(np.ravel(np.nan_to_num(comp)))\n",
    "    if len(details) == 0:\n",
    "        return arr.copy()\n",
    "    details = np.concatenate(details)\n",
    "    mad = np.median(np.abs(details - np.median(details)))\n",
    "    sigma = mad / 0.6745 if mad > 0 else 0.0\n",
    "    uthresh = sigma * np.sqrt(2 * np.log(arr.size + 1e-12))\n",
    "    new_coeffs = [coeffs[0]]\n",
    "    for d in coeffs[1:]:\n",
    "        new_level = tuple(pywt.threshold(np.nan_to_num(comp), value=uthresh, mode='soft') for comp in d)\n",
    "        new_coeffs.append(new_level)\n",
    "    den = pywt.waverec2(new_coeffs, wavelet)\n",
    "    den = den[:arr.shape[0], :arr.shape[1]]\n",
    "    return np.nan_to_num(den, nan=arr).astype(np.float32)\n",
    "\n",
    "def denoise_mentor(img, method=DENOISE_METHOD):\n",
    "    if method == \"wiener\":\n",
    "        return scipy_wiener(img, mysize=(5,5)).astype(np.float32)\n",
    "    return denoise_wavelet_safe(img, wavelet='db4', level=2)\n",
    "\n",
    "def preprocess_image_path(path, method=DENOISE_METHOD, size=IMG_SIZE):\n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = to_gray(img)\n",
    "    img = resize_to(img, size=size)\n",
    "    img = normalize_img(img)\n",
    "    den = denoise_mentor(img, method=method)\n",
    "    residual = (img - den).astype(np.float32)\n",
    "    # normalize residual by its max abs to keep values stable (-1..1)\n",
    "    m = np.max(np.abs(residual)) if np.max(np.abs(residual)) > 0 else 1.0\n",
    "    return (residual / m).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d1361ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- dataset scanning ----------------\n",
    "def process_dataset_dir(base_dir, save_path=None):\n",
    "    residuals = {}\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"⚠ Dataset missing: {base_dir}\")\n",
    "        return residuals\n",
    "    scanners = sorted([d for d in safe_listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))])\n",
    "    for scanner in tqdm(scanners, desc=f\"Processing {os.path.basename(base_dir)}\"):\n",
    "        scanner_dir = os.path.join(base_dir, scanner)\n",
    "        collected = []\n",
    "        # detect dpi subfolders\n",
    "        subdirs = [os.path.join(scanner_dir, s) for s in safe_listdir(scanner_dir) if os.path.isdir(os.path.join(scanner_dir, s))]\n",
    "        if subdirs:\n",
    "            for sdir in subdirs:\n",
    "                files = [os.path.join(sdir, f) for f in safe_listdir(sdir) if f.lower().endswith(VALID_EXTS)]\n",
    "                for fp in files:\n",
    "                    r = preprocess_image_path(fp)\n",
    "                    if r is not None:\n",
    "                        collected.append(r)\n",
    "        else:\n",
    "            files = [os.path.join(scanner_dir, f) for f in safe_listdir(scanner_dir) if f.lower().endswith(VALID_EXTS)]\n",
    "            for fp in files:\n",
    "                r = preprocess_image_path(fp)\n",
    "                if r is not None:\n",
    "                    collected.append(r)\n",
    "        residuals[scanner] = collected\n",
    "    if save_path:\n",
    "        save_pickle(residuals, save_path)\n",
    "    return residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1fae571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Official: 100%|██████████| 11/11 [03:04<00:00, 16.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Infosys_AI-Tracefinder\\Notebooks\\official_residuals.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Wikipedia: 100%|██████████| 11/11 [04:37<00:00, 25.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Infosys_AI-Tracefinder\\Notebooks\\wikipedia_residuals.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Flatfield: 100%|██████████| 11/11 [00:01<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Infosys_AI-Tracefinder\\Notebooks\\flatfield_residuals.pkl\n",
      "Official scanners: ['Canon120-1', 'Canon120-2', 'Canon220', 'Canon9000-1', 'Canon9000-2', 'EpsonV370-1', 'EpsonV370-2', 'EpsonV39-1', 'EpsonV39-2', 'EpsonV550']\n",
      "Wikipedia scanners: ['Canon120-1', 'Canon120-2', 'Canon220', 'Canon9000-1', 'Canon9000-2', 'EpsonV370-1', 'EpsonV370-2', 'EpsonV39-1', 'EpsonV39-2', 'EpsonV550']\n",
      "Flatfield scanners: ['Canon120-1', 'Canon120-2', 'Canon220', 'Canon9000-1', 'Canon9000-2', 'EpsonV370-1', 'EpsonV370-2', 'EpsonV39-1', 'EpsonV39-2', 'EpsonV550']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing fingerprints: 100%|██████████| 11/11 [00:00<00:00, 1077.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Infosys_AI-Tracefinder\\Notebooks\\scanner_fingerprints.pkl\n",
      "Saved fingerprints: D:\\Infosys_AI-Tracefinder\\Notebooks\\scanner_fingerprints.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    " #---------------- run scanning for all 3 datasets ----------------#\n",
    "OFFICIAL_OUT = os.path.join(OUTPUT_DIR, \"official_residuals.pkl\")\n",
    "WIKI_OUT     = os.path.join(OUTPUT_DIR, \"wikipedia_residuals.pkl\")\n",
    "FLATFIELD_OUT= os.path.join(OUTPUT_DIR, \"flatfield_residuals.pkl\")\n",
    "\n",
    "official_residuals = process_dataset_dir(OFFICIAL_DIR, save_path=OFFICIAL_OUT)\n",
    "wikipedia_residuals = process_dataset_dir(WIKI_DIR, save_path=WIKI_OUT)\n",
    "if os.path.exists(FLATFIELD_DIR):\n",
    "    flatfield_residuals = process_dataset_dir(FLATFIELD_DIR, save_path=FLATFIELD_OUT)\n",
    "else:\n",
    "    flatfield_residuals = {}\n",
    "    print(\"Flatfield folder missing; continuing without flatfield.\")\n",
    "\n",
    "print(\"Official scanners:\", list(official_residuals.keys())[:10])\n",
    "print(\"Wikipedia scanners:\", list(wikipedia_residuals.keys())[:10])\n",
    "print(\"Flatfield scanners:\", list(flatfield_residuals.keys())[:10])\n",
    "\n",
    "\n",
    "# ---------------- compute fingerprints ----------------\n",
    "FP_OUT = os.path.join(OUTPUT_DIR, \"scanner_fingerprints.pkl\")\n",
    "FP_KEYS_OUT = os.path.join(OUTPUT_DIR, \"fp_keys.npy\")\n",
    "\n",
    "scanner_fps = {}\n",
    "if flatfield_residuals:\n",
    "    for scanner, res_list in tqdm(flatfield_residuals.items(), desc=\"Computing fingerprints\"):\n",
    "        if not res_list:\n",
    "            scanner_fps[scanner] = np.zeros(IMG_SIZE, dtype=np.float32)\n",
    "            continue\n",
    "        stack = np.stack(res_list, axis=0)\n",
    "        scanner_fps[scanner] = np.mean(stack, axis=0).astype(np.float32)\n",
    "else:\n",
    "    # No flatfield available: create zero fingerprints for union of scanners found in official/wiki\n",
    "    all_scanners = sorted(set(list(official_residuals.keys()) + list(wikipedia_residuals.keys())))\n",
    "    print(\"No flatfield found: creating zero fingerprints for scanners:\", len(all_scanners))\n",
    "    for sc in all_scanners:\n",
    "        scanner_fps[sc] = np.zeros(IMG_SIZE, dtype=np.float32)\n",
    "\n",
    "save_pickle(scanner_fps, FP_OUT)\n",
    "np.save(FP_KEYS_OUT, np.array(sorted(list(scanner_fps.keys()))))\n",
    "print(\"Saved fingerprints:\", FP_OUT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cd06181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- feature extraction helpers (improved) ----------------\n",
    "def corr2d(a, b):\n",
    "    a = a.astype(np.float32).ravel()\n",
    "    b = b.astype(np.float32).ravel()\n",
    "    a -= a.mean(); b -= b.mean()\n",
    "    denom = np.linalg.norm(a) * np.linalg.norm(b)\n",
    "    return float((a @ b) / denom) if denom > 0 else 0.0\n",
    "\n",
    "def fft_radial_energy(img, K=6, log_scale=True):\n",
    "    f = fftshift(fft2(img))\n",
    "    mag = np.abs(f)\n",
    "    if log_scale:\n",
    "        mag = 20 * np.log1p(mag)\n",
    "    h, w = mag.shape\n",
    "    cy, cx = h//2, w//2\n",
    "    yy, xx = np.ogrid[:h, :w]\n",
    "    r = np.sqrt((yy - cy)**2 + (xx - cx)**2)\n",
    "    bins = np.linspace(0, r.max()+1e-12, K+1)\n",
    "    feats = []\n",
    "    for i in range(K):\n",
    "        mask = (r >= bins[i]) & (r < bins[i+1])\n",
    "        sel = mag[mask]\n",
    "        feats.append(float(np.mean(sel)) if sel.size else 0.0)\n",
    "    return feats\n",
    "\n",
    "def lbp_hist_safe(img, P=8, R=1.0, bins=None):\n",
    "    if bins is None:\n",
    "        bins = P + 2   # classic 'uniform' bins\n",
    "    rng = float(img.max() - img.min())\n",
    "    if rng < 1e-12:\n",
    "        g8 = np.zeros_like(img, dtype=np.uint8)\n",
    "    else:\n",
    "        g8 = ((img - img.min()) / (rng + 1e-12) * 255).astype(np.uint8)\n",
    "    codes = local_binary_pattern(g8, P, R, method=\"uniform\")\n",
    "    hist, _ = np.histogram(codes.ravel(), bins=np.arange(bins+1), density=True)\n",
    "    return hist.astype(np.float32).tolist()\n",
    "\n",
    "def glcm_features(img, distances=(1,), angles=(0,)):\n",
    "    # img expected float-like 0..1 or similar; convert to uint8 for graycomatrix\n",
    "    try:\n",
    "        g8 = img_as_ubyte((img - img.min()) / ((img.max() - img.min()) + 1e-12))\n",
    "    except Exception:\n",
    "        g8 = img_as_ubyte(np.clip(img, 0.0, 1.0))\n",
    "    try:\n",
    "        glcm = graycomatrix(g8, distances=distances, angles=angles, levels=256, symmetric=True, normed=True)\n",
    "        contrast = float(graycoprops(glcm, 'contrast')[0,0])\n",
    "        homogeneity = float(graycoprops(glcm, 'homogeneity')[0,0])\n",
    "        energy = float(graycoprops(glcm, 'energy')[0,0])\n",
    "        correlation = float(graycoprops(glcm, 'correlation')[0,0])\n",
    "    except Exception:\n",
    "        return [0.0, 0.0, 0.0, 0.0]\n",
    "    return [contrast, homogeneity, energy, correlation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ace12abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined scanners count: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features (official+wiki): 100%|██████████| 11/11 [01:54<00:00, 10.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Infosys_AI-Tracefinder\\Notebooks\\combined_features.pkl\n",
      "Feature vector length: 31 samples: 4567\n",
      "X_img shape: (4567, 256, 256, 1)\n",
      "features shape: (4567, 31)\n",
      "num labels: (4567,)\n",
      "Saved: X_img.npy, X_feat.npy, y.npy and encoder/scaler in D:\\Infosys_AI-Tracefinder\\Notebooks\n",
      "official_residuals.pkl -> FOUND\n",
      "wikipedia_residuals.pkl -> FOUND\n",
      "flatfield_residuals.pkl -> FOUND\n",
      "scanner_fingerprints.pkl -> FOUND\n",
      "fp_keys.npy -> FOUND\n",
      "combined_features.pkl -> FOUND\n",
      "X_img.npy -> FOUND\n",
      "X_feat.npy -> FOUND\n",
      "y.npy -> FOUND\n",
      "Classes: ['Canon120-1', 'Canon120-2', 'Canon220', 'Canon9000-1', 'Canon9000-2', 'EpsonV370-1', 'EpsonV370-2', 'EpsonV39-1', 'EpsonV39-2', 'EpsonV550', 'HP']\n",
      "Feature vector length saved: 31\n"
     ]
    }
   ],
   "source": [
    "# ---------------- combine Official + Wikipedia residuals and extract features ----------------\n",
    "combined_residuals = {}\n",
    "# merge official then wiki (so order is predictable)\n",
    "for sc, lst in official_residuals.items():\n",
    "    combined_residuals.setdefault(sc, []).extend(lst)\n",
    "for sc, lst in wikipedia_residuals.items():\n",
    "    combined_residuals.setdefault(sc, []).extend(lst)\n",
    "\n",
    "print(\"Combined scanners count:\", len(combined_residuals))\n",
    "\n",
    "fp_keys = sorted(list(scanner_fps.keys()))\n",
    "features = []\n",
    "labels = []\n",
    "img_list = []\n",
    "\n",
    "for scanner, res_list in tqdm(combined_residuals.items(), desc=\"Extracting features (official+wiki)\"):\n",
    "    for res in res_list:\n",
    "        # ensure shapes\n",
    "        if res is None:\n",
    "            continue\n",
    "        # correlation features across fingerprints (11 typically)\n",
    "        v_corr = [corr2d(res, scanner_fps.get(k, np.zeros_like(res))) for k in fp_keys]\n",
    "        # FFT radial energies (6)\n",
    "        v_fft = fft_radial_energy(res, K=6, log_scale=True)\n",
    "        # LBP (10 bins using P=8 uniform => P+2=10)\n",
    "        v_lbp = lbp_hist_safe(res, P=8, R=1.0, bins=10)\n",
    "        # GLCM four features\n",
    "        v_glcm = glcm_features(res, distances=(1,), angles=(0,))\n",
    "        # final feature vector\n",
    "        feat_vec = v_corr + v_fft + v_lbp + v_glcm\n",
    "        features.append(feat_vec)\n",
    "        labels.append(scanner)\n",
    "        img_list.append(np.expand_dims(res, -1))  # H,W,1\n",
    "\n",
    "if len(features) == 0:\n",
    "    raise RuntimeError(\"No features extracted — check dataset folders and image readability.\")\n",
    "\n",
    "FEATURES_OUT = os.path.join(OUTPUT_DIR, \"combined_features.pkl\")\n",
    "save_pickle({\"features\": features, \"labels\": labels, \"fp_keys\": fp_keys}, FEATURES_OUT)\n",
    "print(\"Feature vector length:\", len(features[0]), \"samples:\", len(features))\n",
    "\n",
    "\n",
    "# ---------------- prepare final arrays and encoders ----------------\n",
    "features = np.array(features, dtype=np.float32)\n",
    "labels = np.array(labels, dtype=object)\n",
    "X_img = np.array(img_list, dtype=np.float32)\n",
    "y_raw = np.array(labels, dtype=object)\n",
    "\n",
    "print(\"X_img shape:\", X_img.shape)\n",
    "print(\"features shape:\", features.shape)\n",
    "print(\"num labels:\", y_raw.shape)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_raw)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_feat_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Save final artifacts (Notebooks/ used by training & backend inference)\n",
    "np.save(os.path.join(OUTPUT_DIR, \"X_img.npy\"), X_img)\n",
    "np.save(os.path.join(OUTPUT_DIR, \"X_feat.npy\"), X_feat_scaled)\n",
    "np.save(os.path.join(OUTPUT_DIR, \"y.npy\"), y_encoded)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"hybrid_label_encoder.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "with open(os.path.join(OUTPUT_DIR, \"hybrid_feat_scaler.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Saved: X_img.npy, X_feat.npy, y.npy and encoder/scaler in\", OUTPUT_DIR)\n",
    "\n",
    "\n",
    "# ---------------- sanity checks & diagnostics ----------------\n",
    "for p in [\"official_residuals.pkl\", \"wikipedia_residuals.pkl\", \"flatfield_residuals.pkl\",\n",
    "          \"scanner_fingerprints.pkl\", \"fp_keys.npy\", \"combined_features.pkl\", \"X_img.npy\", \"X_feat.npy\", \"y.npy\"]:\n",
    "    full = os.path.join(OUTPUT_DIR, p)\n",
    "    print(p, \"->\", \"FOUND\" if os.path.exists(full) else \"MISSING\")\n",
    "\n",
    "print(\"Classes:\", list(le.classes_))\n",
    "print(\"Feature vector length saved:\", X_feat_scaled.shape[1])\n",
    "\n",
    "\n",
    "# ---------------- quick diagnostic helper (optional) ----------------\n",
    "def diagnostic_check_data_dirs(data_root=r\"D:\\Infosys_AI-Tracefinder\\Data\"):\n",
    "    VALID_EXTS = (\".tif\", \".tiff\", \".png\", \".jpg\", \".jpeg\")\n",
    "    CHECK_DIRS = {\n",
    "        \"Official\": os.path.join(data_root, \"Official\"),\n",
    "        \"Wikipedia\": os.path.join(data_root, \"Wikipedia\"),\n",
    "        \"Flatfield\": os.path.join(data_root, \"Flatfield\")\n",
    "    }\n",
    "    def find_scanner_files(base):\n",
    "        scanners = sorted([d for d in os.listdir(base) if os.path.isdir(os.path.join(base,d))])\n",
    "        info = {}\n",
    "        for sc in scanners:\n",
    "            scpath = os.path.join(base, sc)\n",
    "            candidates = []\n",
    "            subdirs = [d for d in os.listdir(scpath) if os.path.isdir(os.path.join(scpath, d))]\n",
    "            if subdirs:\n",
    "                for s in subdirs:\n",
    "                    sdpath = os.path.join(scpath, s)\n",
    "                    for f in os.listdir(sdpath):\n",
    "                        if f.lower().endswith(VALID_EXTS):\n",
    "                            candidates.append(os.path.join(sdpath, f))\n",
    "            else:\n",
    "                for f in os.listdir(scpath):\n",
    "                    if f.lower().endswith(VALID_EXTS):\n",
    "                        candidates.append(os.path.join(scpath, f))\n",
    "            info[sc] = candidates\n",
    "        return info\n",
    "\n",
    "    for name, path in CHECK_DIRS.items():\n",
    "        print(\"\\n===\", name, \"at\", path, \"===\")\n",
    "        if not os.path.exists(path):\n",
    "            print(\"MISSING:\", path); continue\n",
    "        info = find_scanner_files(path)\n",
    "        for sc, files in sorted(info.items(), key=lambda x: -len(x[1])):\n",
    "            print(f\"{sc:20s} => {len(files):4d} files\", end=\"\")\n",
    "            if len(files)>0:\n",
    "                print(\"  sample:\", files[0])\n",
    "            else:\n",
    "                print()\n",
    "        if \"HP\" in info:\n",
    "            files = info[\"HP\"]\n",
    "            print(\"\\nHP present: count =\", len(files))\n",
    "            unreadable = []\n",
    "            for f in files:\n",
    "                img = cv2.imread(f, cv2.IMREAD_UNCHANGED)\n",
    "                if img is None:\n",
    "                    unreadable.append(f)\n",
    "            if unreadable:\n",
    "                print(\"Unreadable HP files (cv2.imread returned None):\", len(unreadable))\n",
    "            else:\n",
    "                if files:\n",
    "                    print(\"All HP files readable by OpenCV. First HP file preview:\", files[0])\n",
    "\n",
    "# Call diagnostic if you want:\n",
    "# diagnostic_check_data_dirs()\n",
    "\n",
    "# ---------------- END ----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
